[
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "Oscar Chullo Puclla",
    "section": "",
    "text": "Hola, mi nombre es Oscar Chullo Puclla, soy matemático - estadístico. Me dedico a analizar datos y extraer información relevante para la toma de decisiones. Me apasiona la estadística y me gusta estar al día de las últimas tendencias y herramientas como actualmemte lo es Quarto.\nMe caracteriza la responsabilidad, el orden y la libertad en la exploración de nuevos recursos en los proyectos que me proponga. Vengo trabajando en muchos proyectos como en el sector financiero, donde he trabajado en temáticas de riesgo crediticio, además de otros proyectos aplicativos de machine-learning.\nTengo el objetivo de ayudar a las poblaciones u organizaciones a mejorar su rendimiento y optimizar sus recursos mediante el uso de técnicas estadísticas avanzadas, asi como tambien de guiar a futuros profesionales.\nPor ello, construí el presente website como producto de los conocimientos que adquiero aprendiendo."
  },
  {
    "objectID": "About.html#presentación",
    "href": "About.html#presentación",
    "title": "Oscar Chullo Puclla",
    "section": "",
    "text": "Hola, mi nombre es Oscar Chullo Puclla, soy matemático - estadístico. Me dedico a analizar datos y extraer información relevante para la toma de decisiones. Me apasiona la estadística y me gusta estar al día de las últimas tendencias y herramientas como actualmemte lo es Quarto.\nMe caracteriza la responsabilidad, el orden y la libertad en la exploración de nuevos recursos en los proyectos que me proponga. Vengo trabajando en muchos proyectos como en el sector financiero, donde he trabajado en temáticas de riesgo crediticio, además de otros proyectos aplicativos de machine-learning.\nTengo el objetivo de ayudar a las poblaciones u organizaciones a mejorar su rendimiento y optimizar sus recursos mediante el uso de técnicas estadísticas avanzadas, asi como tambien de guiar a futuros profesionales.\nPor ello, construí el presente website como producto de los conocimientos que adquiero aprendiendo."
  },
  {
    "objectID": "About.html#educación",
    "href": "About.html#educación",
    "title": "Oscar Chullo Puclla",
    "section": "Educación",
    "text": "Educación\nUniversidad Nacional de San Antonio Abad del Cusco | Cusco, Perú\nEstudiante de Matemáticas - Estadística | Setiembre 2019 - Actualidad"
  },
  {
    "objectID": "About.html#habilidades",
    "href": "About.html#habilidades",
    "title": "Oscar Chullo Puclla",
    "section": "Habilidades",
    "text": "Habilidades\nR | Programación - Estadística - Datos | 2022 - Actualidad\nRmarkdown | Redacción académica y científica | 2023 - Actualidad\nQuarto | Posit - Quarto markdown | 2023 - Actualidad\nPython | Programación - Estadística | 2023 - Actualidad"
  },
  {
    "objectID": "About.html#experience",
    "href": "About.html#experience",
    "title": "Oscar Chullo Puclla",
    "section": "Experience",
    "text": "Experience\nGit Page | Website | 2023 - Actualidad\nStack Overflow | Respuestas | 2023 - Actualidad"
  },
  {
    "objectID": "html/prueba.html",
    "href": "html/prueba.html",
    "title": "Evento de Prueba 1",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\nYou can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot.\n\n\n\nasdas"
  },
  {
    "objectID": "html/prueba.html#including-plots",
    "href": "html/prueba.html#including-plots",
    "title": "Evento de Prueba 1",
    "section": "",
    "text": "You can also embed plots, for example:\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "html/prueba.html#hola-baby",
    "href": "html/prueba.html#hola-baby",
    "title": "Evento de Prueba 1",
    "section": "",
    "text": "asdas"
  },
  {
    "objectID": "html/prueba.html#sadasd",
    "href": "html/prueba.html#sadasd",
    "title": "Evento de Prueba 1",
    "section": "sadasd",
    "text": "sadasd\nsadasd"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hola, bienvenido a mi website.",
    "section": "",
    "text": "Aquí podrás encontrar información sobre diversos temas, como R, Python, Machine-Learning y más. También podrás interactuar conmigo por los enlaces en la barra de herramientas y pedirme que te ayude en algunas temáticas, crear contenido creativo o mejorar.\n\n\n\n \n  \n   \n  \n    \n     twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LindedIn"
  },
  {
    "objectID": "Post/Multivariado/AnalisisCorrSimple.html",
    "href": "Post/Multivariado/AnalisisCorrSimple.html",
    "title": "Análisis de Correspondencia Simple",
    "section": "",
    "text": "ACS\nEl objetivo de algunas de las técnicas multivariadas consiste en explicar con un menor número de dimensiones (factores o componentes), la información inicial (Principio de parsimonia).\nCuando las variables son continuas o medidas en escala de intervalo o de razón, el PCA o el modelo de Análisis Factorial son los procedimientos apropiados para analizar la interdependencia de un conjunto de variables.\nEn cambio, cuando las variables son cualitativas, es necesario acudir al Análisis de Correspondencia Simple (caso dos variables) o al Análisis de Correspondencia Múltiple (tres o más variables). En esta técnica lo que se busca es encontrar la relación que exista entre las categorías de una variable con las categorías de otra(s) variable(s).\n\nPrueba de independencia\nResulta necesario identificar si existen asociaciones entre las categorías, ya que en caso de no existir asociaciones no resulta conveniente hacer uso de la técnica de ACS.\n\n\n\nEjemplo Práctico\nEjemplo extraidon de Luque, T. (2000). Técnicas de Análisis de Datos en Investigación de Mercados. Ediciones Pirámide.\n\nDatos\nLos datos corresponden a una encuesta que se realizó a 500 personas en relación a su opinión con respecto al sistema sanitario público. Se registró adicionalmente, el tipo de renta percibido por cada encuestado.\nOpinión\n\ndatos.acs &lt;- matrix(c(75,40,35,60,50,70,20,40,30,15,40,25),nrow=4,byrow=T)\ndimnames(datos.acs)&lt;- list(renta=c(\"Bajo\",\"Medio\",\"Alto\", \"Muy Alto\"), opinion=c(\"Bueno\",\"Malo\",\"Regular\"))\ndatos.acs\n\n          opinion\nrenta      Bueno Malo Regular\n  Bajo        75   40      35\n  Medio       60   50      70\n  Alto        20   40      30\n  Muy Alto    15   40      25\n\n\nUso del package kableExtra para organizar visualmente los datos\n\nlibrary(kableExtra)\nlibrary(magrittr)\ndatos.acs %&gt;% kable() %&gt;% kable_styling()\n\n\n\n\n\nBueno\nMalo\nRegular\n\n\n\n\nBajo\n75\n40\n35\n\n\nMedio\n60\n50\n70\n\n\nAlto\n20\n40\n30\n\n\nMuy Alto\n15\n40\n25\n\n\n\n\n\n\n\n\naddmargins(datos.acs)%&gt;% kable() %&gt;% kable_styling()\n\n\n\n\n\nBueno\nMalo\nRegular\nSum\n\n\n\n\nBajo\n75\n40\n35\n150\n\n\nMedio\n60\n50\n70\n180\n\n\nAlto\n20\n40\n30\n90\n\n\nMuy Alto\n15\n40\n25\n80\n\n\nSum\n170\n170\n160\n500\n\n\n\n\n\n\n\n\n\nVisualización de una Tabla de Contingencia usando una matriz gráfica\nCon las gráficas se analizará la asociación entre las variables\n\n\nVisualización por Balloonplots\n\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n# Convertir los datos en una tabla\ndt &lt;- as.table(datos.acs)\ndt &lt;- prop.table(dt,margin=1); dt\n\n          opinion\nrenta          Bueno      Malo   Regular\n  Bajo     0.5000000 0.2666667 0.2333333\n  Medio    0.3333333 0.2777778 0.3888889\n  Alto     0.2222222 0.4444444 0.3333333\n  Muy Alto 0.1875000 0.5000000 0.3125000\n\n\n\nballoonplot(t(dt), \n            main =\"Gráfico Opinión Renta\", \n            xlab =\"Opinión\", \n            ylab=\"Renta\",\n            label = F, cum.margins=F, \n            label.lines=F, show.margins = FALSE)\n\n\n\n\n\n\nVisualización por Mosaicos\n\nlibrary(graphics)\nmosaicplot(t(dt),shade=F)\n\n\n\n\n\n\nPrueba de Independencia Chi-Cuadrado\nContrastamos la hipótesis nula de independencia entre las dos variables que conforman la tabla de contingencia.\n\nHo: Las variables son independientes\nHa: Las variables son dependientes\n\n\nprueba &lt;- chisq.test(datos.acs);prueba\n\n\n    Pearson's Chi-squared test\n\ndata:  datos.acs\nX-squared = 40.049, df = 6, p-value = 4.455e-07\n\n\n\n# Frecuencia Relativa (fij)\nprop.table(datos.acs)\n\n          opinion\nrenta      Bueno Malo Regular\n  Bajo      0.15 0.08    0.07\n  Medio     0.12 0.10    0.14\n  Alto      0.04 0.08    0.06\n  Muy Alto  0.03 0.08    0.05\n\n\n\n\n\nLibrary(FactoMineR)\nCon la función CA de FactoMineR se obtienen los autovalores y la prueba de independencia de Chi Cuadrado.\n\nlibrary(FactoMineR) \n#3 filas y 4 columnas min(3,4)-1 -&gt; ncp=2\nres.ca &lt;- CA(datos.acs,ncp=2,graph=F)\nres.ca \n\n**Results of the Correspondence Analysis (CA)**\nThe row variable has  4  categories; the column variable has 3 categories\nThe chi square of independence between the two variables is equal to 40.04927 (p-value =  4.454704e-07 ).\n*The results are available in the following objects:\n\n   name              description                   \n1  \"$eig\"            \"eigenvalues\"                 \n2  \"$col\"            \"results for the columns\"     \n3  \"$col$coord\"      \"coord. for the columns\"      \n4  \"$col$cos2\"       \"cos2 for the columns\"        \n5  \"$col$contrib\"    \"contributions of the columns\"\n6  \"$row\"            \"results for the rows\"        \n7  \"$row$coord\"      \"coord. for the rows\"         \n8  \"$row$cos2\"       \"cos2 for the rows\"           \n9  \"$row$contrib\"    \"contributions of the rows\"   \n10 \"$call\"           \"summary called parameters\"   \n11 \"$call$marge.col\" \"weights of the columns\"      \n12 \"$call$marge.row\" \"weights of the rows\"         \n\n\n\n# Scree Plot de los Autovalores\nres.ca$eig\n\n      eigenvalue percentage of variance cumulative percentage of variance\ndim 1 0.06510303               81.27866                          81.27866\ndim 2 0.01499552               18.72134                         100.00000\n\n\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_screeplot(res.ca, addlabels = TRUE, ylim = c(0, 90))\n\n\n\n\n\nGráficos- Biplot\n\nplot.CA(res.ca) \n\n\n\n\n\nplot.CA(res.ca, axes = c(1,2), col.row = \"blue\", col.col = \"red\")\n\n\n\n\n\nplot.CA(res.ca,mass=c(T,T))\n\n\n\n\n\nfviz_ca_biplot(res.ca, repel = T)\n\n\n\n\n\n\nIndicadores ACS\n\nsummary(res.ca,nb.dec = 3, ncp = 2)\n\n\nCall:\nCA(X = datos.acs, ncp = 2, graph = F) \n\nThe chi square of independence between the two variables is equal to 40.04927 (p-value =  4.454704e-07 ).\n\nEigenvalues\n                       Dim.1   Dim.2\nVariance               0.065   0.015\n% of var.             81.279  18.721\nCumulative % of var.  81.279 100.000\n\nRows\n           Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nBajo     |    34.375 | -0.322 47.655  0.903 | -0.106 22.341  0.097 |\nMedio    |     9.485 | -0.027  0.391  0.027 |  0.160 61.558  0.973 |\nAlto     |    13.219 |  0.268 19.803  0.975 | -0.043  2.178  0.025 |\nMuy Alto |    23.019 |  0.362 32.151  0.909 | -0.114 13.923  0.091 |\n\nColumns\n           Iner*1000    Dim.1    ctr   cos2    Dim.2    ctr   cos2  \nBueno    |    40.923 | -0.344 61.919  0.985 | -0.042  4.081  0.015 |\nMalo     |    26.667 |  0.253 33.467  0.817 | -0.120 32.533  0.183 |\nRegular  |    12.509 |  0.097  4.614  0.240 |  0.172 63.386  0.760 |\n\n\n\n# Coordenadas de las Dimensiones para filas y columnas \nrow &lt;- get_ca_row(res.ca)\ncol &lt;- get_ca_col(res.ca)\nhead(row$coord)\n\n               Dim 1       Dim 2\nBajo     -0.32158412 -0.10567395\nMedio    -0.02657899  0.16012994\nAlto      0.26762688 -0.04259682\nMuy Alto  0.36169272 -0.11423227\n\n\n\nhead(col$coord)\n\n              Dim 1       Dim 2\nBueno   -0.34432911 -0.04242433\nMalo     0.25314619 -0.11978473\nRegular  0.09688186  0.17234713\n\n\n\n# Gráficos de las contribuciones absolutas de las filas y columnas a cada dimensión\nhead(row$contrib)\n\n              Dim 1     Dim 2\nBajo     47.6550858 22.340641\nMedio     0.3906413 61.558217\nAlto     19.8029896  2.178037\nMuy Alto 32.1512832 13.923105\n\n\n\nhead(col$contrib)\n\n            Dim 1     Dim 2\nBueno   61.919181  4.080819\nMalo    33.467286 32.532714\nRegular  4.613534 63.386466\n\n\n\nfviz_contrib(res.ca, choice = \"row\", axes = 1)\n\n\n\n\n\nfviz_contrib(res.ca, choice = \"row\", axes = 2)\n\n\n\n\n\nfviz_contrib(res.ca, choice = \"col\", axes = 1)\n\n\n\n\n\nfviz_contrib(res.ca, choice = \"col\", axes = 2)"
  },
  {
    "objectID": "Post/Multivariado/AnDisc.html",
    "href": "Post/Multivariado/AnDisc.html",
    "title": "Análisis Discriminante",
    "section": "",
    "text": "El Análisis Discriminante (AD),es una técnica de predicción en la pertenencia a un grupo (variable dependiente) a partir de un conjunto de predictores (variables independientes). El objetivo del AD es entender las diferencias de los grupos y predecir la verosimilitud de que una persona o un objeto pertenezca a una clase o grupo basándose en los valores que toma en los predictores.\nExisten dos enfoques en la clasificación discriminante:\nEl primer enfoque esta basado en conseguir, a partir de las variables explicativas, unas funciones lineales de éstas con la capacidad de clasificar a otros individuos, donde la función de mayor valor define el grupo al que pertenece de la forma más probable.\nEl AD solo admite variables cuantitativas como regresoras, por lo que si alguna de las variables independientes es categórica, hay que utilizar otros métodos alternativos de clasificación.\nRevisar también: https://rpubs.com/JairoAyala/CAD."
  },
  {
    "objectID": "Post/Multivariado/AnDisc.html#análisis-discriminante-lineal",
    "href": "Post/Multivariado/AnDisc.html#análisis-discriminante-lineal",
    "title": "Análisis Discriminante",
    "section": "Análisis Discriminante Lineal",
    "text": "Análisis Discriminante Lineal\nEl análisis discriminante lineal (LDA) es un método de clasificación de aprendizaje automático supervisado (binario o multimonial) y un método de reducción de dimensiones. EL LDA encuentra combinaciones lineales de variables que mejor discriminan las clases de la variable respuesta.\nUn enfoque supone que las variables predictoras son variables aleatorias continuas normalmente distribuidas y con la misma varianza. Para que se cumpla esta condición, se deberá escalar los datos.\nPara una variable respuesta de k niveles, LDA produce k−1 (reglas) discriminantes utilizando el teorema de Bayes.\n\\[Pr[Y=C_l|X]=\\frac{P[Y=C_l]P[X|Y=C_l]}{\\sum}\\]\nDonde Y es la variable respuesta, X son los predictores y \\(C_l\\) es la clase \\(l-ésima\\). Entonces, la probablidad de que \\(Y\\) sea igual al nivel \\(C_l\\) dados los predictores \\(X\\) es igual a la probabilidad a priori de \\(Y\\) multiplicado por la probabilidad de observar \\(X\\) si \\(Y=C_l\\) dividido por la suma de todas las probabilidades de \\(X\\) data las priors. El valor predicho para cualquier \\(X\\) es simplemente \\(C_l\\) que tenga la probabilidad máxima.\nUna forma de calcular las probabilidades es asumir que \\(X\\) tiene una distribución normal multivariante con medias \\(\\mu_l\\) y varianza común \\(\\sum\\) . Entonces la función discriminante lineal para el grupo \\(l\\) es:\n\\[X´{\\sum}^{-1}\\mu_l-0.5{\\mu}´_l{\\sum}^{-1}\\mu_l+log(Pr[Y=C_l])\\]\nLa media teórica y la matriz de covarianza se estiman mediante la media muestral \\(\\mu=\\bar{x_l}\\) y la covarianza \\(\\sum=S\\), y los predictores \\(X\\) se reemplazan con los predictores de muestra que denotamos \\(\\mu\\).\n\nEjemplo Aplicativo\nUn equipo de biólogos quiere generar un modelo estadístico que permita identificar a que especie (a o b) pertenece un determinado insecto. Para ello se han medido tres variables (longitud de las patas, diámetro del abdomen y diámetro del órgano sexual) en 10 individuos de cada una de las dos especies.\nObtención de los datos de entrenamiento\n\ninput &lt;- (\"\nespecie pata abdomen organo_sexual \na 191 131 53\na 185 134 50\na 200 137 52\na 173 127 50\na 171 128 49\na 160 118 47\na 188 134 54\na 186 129 51\na 174 131 52\na 163 115 47\nb 186 107 49\nb 211 122 49\nb 201 144 47\nb 242 131 54\nb 184 108 43\nb 211 118 51\nb 217 122 49\nb 223 127 51\nb 208 125 50\nb 199 124 46\n\")\n\n\ndatos &lt;- read.table(textConnection(input), header = TRUE)\ndatos$especie &lt;- as.factor(datos$especie)\n\n\nExploración gráfica de los datos\n\nlibrary(ggplot2)\nlibrary(ggpubr)\np1 &lt;- ggplot(data = datos, aes(x = pata, fill = especie)) + geom_histogram(position = \"identity\", alpha = 0.5)\np2 &lt;- ggplot(data = datos, aes(x = abdomen, fill = especie)) + geom_histogram(position = \"identity\", alpha = 0.5)\np3 &lt;- ggplot(data = datos, aes(x = organo_sexual, fill = especie)) + geom_histogram(position = \"identity\", alpha = 0.5)\n\nggarrange(p1, p2, p3, nrow = 3, common.legend = TRUE)\n\n\n\n\nA nivel individual, la longitud de la pata parece ser la variable que más diferencia a las especies a y b.\n\n\nCorrelación entre variables\n\npairs(x = datos[, c(\"pata\",\"abdomen\",\"organo_sexual\")],\n      col = c(\"firebrick\", \"green3\")[datos$especie], pch = 19)\n\n\n\n\nLos pares de variables abdomen-pata y pata-organo_sexual presentan caracteristicas que evidencian una diferenciación entre las especies a y b.\n\n\nGráfico tridimensional\n\nlibrary(scatterplot3d)\nscatterplot3d(datos$pata, datos$abdomen, datos$organo_sexual,\n              color = c(\"firebrick\", \"green3\")[datos$especie], pch = 19,\n              grid = TRUE, xlab = \"pata\", ylab = \"abdomen\",\n              zlab = \"organo sexual\", angle = 65, cex.axis = 0.6)\nlegend(\"topleft\",\n       bty = \"n\", cex = 0.8,\n       title = \"Especie\",\n       c(\"a\", \"b\"), fill = c(\"firebrick\", \"green3\"))\n\n\n\n\nLa representación de las tres variables simultáneamente, nos indican que las dos especies están bastante separadas en un espacio 3D generado para su representación.\n\n\nPrior probabilities\nComo no se dispone de información acerca de la abundancia relativa de las especies a y b, a nivel poblacional, se considera como probabilidad previa de cada especie el número de observaciones de la especie entre el número de observaciones totales; teniendose:\n\\[\\hat{\\pi_a}=\\hat{\\pi_b}=10/20=0.5  \\longrightarrow 50\\% \\]\n\n# Representación de cuantiles normales de cada variable para cada especie \nfor (k in 2:4) {\n  j0 &lt;- names(datos)[k]\n  x0 &lt;- seq(min(datos[, k]), max(datos[, k]), le = 50)\n  for (i in 1:2) {\n    i0 &lt;- levels(datos$especie)[i]\n    x &lt;- datos[datos$especie == i0, j0]\n    qqnorm(x, main = paste(\"especie\", i0, j0), pch = 19, col = i + 1)\n    qqline(x)\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Contraste de normalidad Shapiro-Wilk para cada variable en cada especie\nlibrary(reshape2)\nlibrary(knitr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(magrittr)\ndatos_tidy &lt;- melt(datos, value.name = \"valor\")\n\nUsing especie as id variables\n\nkable(datos_tidy %&gt;% group_by(especie, variable) %&gt;% summarise(p_value_Shapiro.test = shapiro.test(valor)$p.value))\n\n`summarise()` has grouped output by 'especie'. You can override using the\n`.groups` argument.\n\n\n\n\n\nespecie\nvariable\np_value_Shapiro.test\n\n\n\n\na\npata\n0.7763034\n\n\na\nabdomen\n0.1845349\n\n\na\norgano_sexual\n0.6430844\n\n\nb\npata\n0.7985711\n\n\nb\nabdomen\n0.5538213\n\n\nb\norgano_sexual\n0.8217855\n\n\n\n\n\nNo hay evidencias de falta de normalidad univariante en de las variables."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html",
    "href": "Post/Multivariado/AnFact.html",
    "title": "Análisis Factorial",
    "section": "",
    "text": "El análisis factorial (FA) es un método estadístico que se utiliza para buscar algunas variables no observadas (latentes) llamadas factores a partir de variables observadas.\nEl análisis factorial se utiliza para analizar las relaciones entre una gran cantidad de variables observadas y para identificar una cantidad menor de variables subyacentes no observadas, que se denominan factores. El objetivo del análisis factorial es reducir la complejidad de un conjunto de datos e identificar la estructura subyacente que explica los datos observados.\nAl reducir el número de variables en el modelo, el análisis factorial puede ayudar a superar problemas de sobreajuste, multicolinealidad y alta dimensionalidad.\n\nEl sobreajuste ocurre cuando un modelo es demasiado complejo y se ajusta demasiado a los datos de entrenamiento, lo que puede provocar un rendimiento deficiente con datos nuevos.\nLa multicolinealidad ocurre cuando dos o más variables están altamente correlacionadas entre sí, lo que puede conducir a estimaciones inestables y poco confiables de los coeficientes del modelo.\nLa alta dimensionalidad ocurre cuando hay demasiadas variables en el modelo, lo que puede conducir a una ineficiencia computacional y una interpretación reducida del modelo.\n\nExisten dos tipos de AF, llamados análisis factorial exploratorio y confirmatorio (EFA y CFA). Tanto EFA como CFA tienen como objetivo reproducir las relaciones observadas entre un grupo de características con un conjunto más pequeño de variables latentes. La EFA se utiliza de manera descriptiva y basada en datos para descubrir qué variables medidas son indicadores razonables de las diversas dimensiones latentes. Por el contrario, el AFC se lleva a cabo de una manera a priori, de prueba de hipótesis que requiere sólidos fundamentos empíricos o teóricos. Aquí nos centraremos principalmente en EFA, que se utiliza para agrupar características en un número específico de factores latentes."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#concepto",
    "href": "Post/Multivariado/AnFact.html#concepto",
    "title": "Análisis Factorial",
    "section": "",
    "text": "El análisis factorial (FA) es un método estadístico que se utiliza para buscar algunas variables no observadas (latentes) llamadas factores a partir de variables observadas.\nEl análisis factorial se utiliza para analizar las relaciones entre una gran cantidad de variables observadas y para identificar una cantidad menor de variables subyacentes no observadas, que se denominan factores. El objetivo del análisis factorial es reducir la complejidad de un conjunto de datos e identificar la estructura subyacente que explica los datos observados.\nAl reducir el número de variables en el modelo, el análisis factorial puede ayudar a superar problemas de sobreajuste, multicolinealidad y alta dimensionalidad.\n\nEl sobreajuste ocurre cuando un modelo es demasiado complejo y se ajusta demasiado a los datos de entrenamiento, lo que puede provocar un rendimiento deficiente con datos nuevos.\nLa multicolinealidad ocurre cuando dos o más variables están altamente correlacionadas entre sí, lo que puede conducir a estimaciones inestables y poco confiables de los coeficientes del modelo.\nLa alta dimensionalidad ocurre cuando hay demasiadas variables en el modelo, lo que puede conducir a una ineficiencia computacional y una interpretación reducida del modelo.\n\nExisten dos tipos de AF, llamados análisis factorial exploratorio y confirmatorio (EFA y CFA). Tanto EFA como CFA tienen como objetivo reproducir las relaciones observadas entre un grupo de características con un conjunto más pequeño de variables latentes. La EFA se utiliza de manera descriptiva y basada en datos para descubrir qué variables medidas son indicadores razonables de las diversas dimensiones latentes. Por el contrario, el AFC se lleva a cabo de una manera a priori, de prueba de hipótesis que requiere sólidos fundamentos empíricos o teóricos. Aquí nos centraremos principalmente en EFA, que se utiliza para agrupar características en un número específico de factores latentes."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#ventajas",
    "href": "Post/Multivariado/AnFact.html#ventajas",
    "title": "Análisis Factorial",
    "section": "Ventajas",
    "text": "Ventajas\n\nFA es una forma útil de combinar diferentes grupos de datos en factores representativos conocidos, reduciendo así la dimensionalidad de un conjunto de datos.\nFA puede tener en cuenta el conocimiento experto de los investigadores al elegir la cantidad de factores a utilizar y puede usarse para identificar variables latentes u ocultas que pueden no ser evidentes al utilizar otros métodos de análisis.\nEs fácil de implementar con muchas herramientas de software disponibles para realizar FA. La FA confirmatoria se puede utilizar para probar hipótesis."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#desventajas",
    "href": "Post/Multivariado/AnFact.html#desventajas",
    "title": "Análisis Factorial",
    "section": "Desventajas",
    "text": "Desventajas\n\nJustificar la elección del número de factores a utilizar puede resultar difícil si se sabe poco sobre la estructura de los datos antes de realizar el análisis.\nA veces, puede resultar difícil interpretar qué significan los factores una vez completado el análisis.\nAl igual que PCA, los métodos estándar para realizar FA suponen que las variables de entrada son continuas, aunque las extensiones de FA permiten incluir variables ordinales y binarias (después de transformar la matriz de entrada)."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#flujo-de-trabajo-en-r",
    "href": "Post/Multivariado/AnFact.html#flujo-de-trabajo-en-r",
    "title": "Análisis Factorial",
    "section": "Flujo de trabajo en R",
    "text": "Flujo de trabajo en R\n\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(psych)\nlibrary(lavaan)\nlibrary(kableExtra)\nlibrary(readxl)\n\nEl conjunto de datos sobre esperanza de vida contiene 31 observaciones de países.\n\nLifEx&lt;-read_xlsx(\"datos/datalife.xlsx\")\nLifEx |&gt;\n  kbl() |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\nCountry\nm0\nm25\nm50\nm75\nw0\nw25\nw50\nw75\n\n\n\n\nArgelia\n63\n51\n30\n13\n67\n54\n34\n15\n\n\nCameroon\n34\n29\n13\n5\n38\n32\n17\n6\n\n\nMadagascar\n38\n30\n17\n7\n38\n34\n20\n7\n\n\nMauritius\n59\n42\n20\n6\n64\n46\n25\n8\n\n\nReunion\n56\n38\n18\n7\n62\n46\n25\n10\n\n\nSeychelles\n62\n44\n24\n7\n69\n50\n28\n14\n\n\nSouth Africa C\n50\n39\n20\n7\n55\n43\n23\n8\n\n\nSouth Africa W\n65\n44\n22\n7\n72\n50\n27\n9\n\n\nTunisia\n56\n46\n24\n11\n63\n54\n33\n19\n\n\nCanadá\n69\n47\n24\n8\n75\n53\n29\n10\n\n\nCosta Rica\n65\n48\n26\n9\n68\n50\n27\n10\n\n\nDominican Rep.\n64\n50\n28\n11\n66\n51\n29\n11\n\n\nEl Salvador\n56\n44\n25\n10\n61\n48\n27\n12\n\n\nGreenland\n60\n44\n22\n6\n65\n45\n25\n9\n\n\nGrenada\n61\n45\n22\n8\n65\n49\n27\n10\n\n\nGuatemala\n49\n40\n22\n9\n51\n41\n23\n8\n\n\nHonduras\n59\n42\n22\n6\n61\n43\n22\n7\n\n\nJamaica\n63\n44\n23\n8\n67\n48\n26\n9\n\n\nMexico\n59\n44\n24\n8\n63\n46\n25\n8\n\n\nNicaragua\n65\n48\n28\n14\n68\n51\n29\n13\n\n\nPanama\n65\n48\n26\n9\n67\n49\n27\n10\n\n\nTrinidad 62\n64\n63\n21\n7\n68\n47\n25\n9\n\n\nTrinidad 67\n64\n43\n21\n6\n68\n47\n24\n8\n\n\nUnited States 66\n67\n45\n23\n8\n74\n51\n28\n10\n\n\nUnited States NW66\n61\n40\n21\n10\n67\n46\n25\n11\n\n\nUnited States W66\n68\n46\n23\n8\n75\n52\n29\n10\n\n\nUnited States 67\n67\n45\n23\n8\n74\n51\n28\n10\n\n\nArgentina\n65\n46\n24\n9\n71\n51\n28\n10\n\n\nChile\n59\n43\n23\n10\n66\n49\n27\n12\n\n\nColombia\n58\n44\n24\n9\n62\n47\n25\n10\n\n\nEcuador\n57\n46\n28\n9\n60\n49\n28\n11"
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#explorando-la-matriz-de-correlación",
    "href": "Post/Multivariado/AnFact.html#explorando-la-matriz-de-correlación",
    "title": "Análisis Factorial",
    "section": "Explorando la matriz de correlación",
    "text": "Explorando la matriz de correlación\nEl insumo del análisis factorial es la matriz de correlación y, para algunos índices, el número de observaciones. Podemos explorar correlaciones con el paquete corrr:\n\ncor_tb &lt;- correlate(LifEx[,-1])\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\ncorrr::rearrange agrupa variables altamente correlacionadas más juntas, y corrr::rplot visualiza el resultado (se ha usado una escala de color personalizada para resaltar los resultados):\n\ncor_tb |&gt;\n  rearrange() |&gt;\n  rplot(colors = c(\"red\", \"white\", \"blue\"))\n\n\n\n\nSe muestran 4 grupos, en torno a las variables:\n\nm0 y w0 (primer grupo).\nm25 y w25 (segundo grupo).\nm50 y w50 (tercer grupo).\nm75 y w75 (cuarto grupo).\n\nComencemos el flujo de trabajo del análisis factorial. Para ello necesitamos obtener las correlaciones en formato matricial con la función cor base R.\n\ncor_mat &lt;- cor(LifEx[,-1])"
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#pruebas-preliminares",
    "href": "Post/Multivariado/AnFact.html#pruebas-preliminares",
    "title": "Análisis Factorial",
    "section": "Pruebas preliminares",
    "text": "Pruebas preliminares\nEn el análisis preliminar, examinamos si la matriz de correlación es adecuada para el análisis factorial. Tenemos dos métodos disponibles:\n\nLa prueba de adecuación de la muestra de Kaiser-Meyer-Olkin (KMO). Según Kaiser (1974), los valores de KMO &gt; 0,9 son maravillosos, en 0,80, meritorios, en 0,70, mediocres, en 0,60, mediocres, en 0,50, miserables y por debajo de 0,5, inaceptables. Ese índice lo podemos obtener con psych::KMO. También proporciona una medida de adecuación de la muestra para cada variable.\nLa prueba de esfericidad de Bartlett. Es una prueba de la hipótesis de que la matriz de correlación es una matriz de identidad. Si se puede rechazar esta hipótesis nula, podemos suponer que las variables están correlacionadas y luego realizar un análisis factorial. Podemos hacer esta prueba con la función psych::cortest.bartlett.\n\nLos resultados de la prueba KMO son:\n\nKMO(cor_mat)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cor_mat)\nOverall MSA =  0.79\nMSA for each item = \n  m0  m25  m50  m75   w0  w25  w50  w75 \n0.69 0.90 0.78 0.86 0.68 0.84 0.86 0.83 \n\n\nEstos resultados pueden considerarse mediocres.\nLos resultados de la prueba de Bartlett son:\n\ncortest.bartlett(R = cor_mat, n = 31)\n\n$chisq\n[1] 372.3232\n\n$p.value\n[1] 7.881036e-62\n\n$df\n[1] 28\n\n\nEstos resultados proporcionan una fuerte evidencia de que las variables están correlacionadas."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#número-de-factores-a-extraer",
    "href": "Post/Multivariado/AnFact.html#número-de-factores-a-extraer",
    "title": "Análisis Factorial",
    "section": "Número de factores a extraer",
    "text": "Número de factores a extraer\nUn punto de partida para decidir cuántos factores extraer es examinar los valores propios de la matriz de correlación.\n\neigen(cor_mat)$values\n\n[1] 5.602410288 1.358181546 0.499327002 0.308125898 0.154689625 0.058633785\n[7] 0.012821625 0.005810232\n\n\nSe utilizan dos criterios para decidir el número de factores a extraer:\n\nElija tantos factores como valores propios sean mayores que uno.\nExamine el punto del codo del diagrama de pedregal. En ese gráfico tenemos el rango de los factores en el eje x y los valores propios en el eje y.\n\nHagamos el diagrama de pedregal para ver el punto del codo.\n\nn &lt;- dim(cor_mat)[1]\nscree_tb &lt;- tibble(x = 1:n, \n                   y = sort(eigen(cor_mat)$value, decreasing = TRUE))\n\nscree_plot &lt;- scree_tb |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(color = \"cyan\") +\n  geom_line(color = \"ForestGreen\") +\n  scale_x_continuous(breaks = 1:n) +\n  ggtitle(\"Scree plot\")\n\nscree_plot\n\n\n\n\n\nscree_plot +\n  geom_vline(xintercept = 4, color = \"magenta\", linetype = \"dashed\") +\n  annotate(\"text\", 4.1, 2, label = \"elbow point\", color = \"magenta\", hjust = 0)\n\n\n\n\nObservamos que el método del codo ocurre con cuatro factores y que tenemos tres factores con valor propio mayor que uno. Se ha optado por obtener una solución de tres factores."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#cargas-factoriales-y-rotaciones",
    "href": "Post/Multivariado/AnFact.html#cargas-factoriales-y-rotaciones",
    "title": "Análisis Factorial",
    "section": "Cargas factoriales y rotaciones",
    "text": "Cargas factoriales y rotaciones\nCargas factoriales \\(f_{ij}\\) están definidos para cada variable \\(i\\) y factor \\(j\\), y representan la correlación entre ambos. Un alto valor de \\(f_{ij}\\) significa que una gran cantidad de variabilidad de la variable \\(i\\) puede explicarse por el factor \\(j\\).\nLas cargas factoriales de un modelo son únicas hasta una rotación. Preferimos métodos de rotación que obtienen cargas factoriales que permiten relacionar un factor con cada variable. Las rotaciones ortogonales producen un conjunto de factores no correlacionados. Los métodos de rotación ortogonal más comunes son:\n\nVarimax maximiza la varianza de las cargas al cuadrado en cada factor, de modo que cada factor tenga solo unas pocas variables con grandes cargas por factor.\nQuartimax minimiza la cantidad de factores necesarios para explicar una variable.\nEquamax es una combinación de varimax y quartimax.\n\nLas rotaciones oblicuas conducen a un conjunto de factores correlacionados. Los métodos de rotación oblicua más comunes son oblimin y promax.\nEn psych::fa, el tipo de rotación se pasa con el parámetro de rotación.\nPara un análisis factorial, podemos dividir la varianza de cada variable en:\n\nComunalidad h2: la proporción de la varianza explicada por factores.\nUnicidad u2: la proporción de la varianza no explicada por factores, por lo tanto única para la variable."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#interpretación-de-los-resultados",
    "href": "Post/Multivariado/AnFact.html#interpretación-de-los-resultados",
    "title": "Análisis Factorial",
    "section": "Interpretación de los resultados",
    "text": "Interpretación de los resultados\nPodemos realizar el análisis factorial utilizando la psych::fa función. Esta función permite varios métodos para realizar el análisis factorial especificando fmel parámetro. Las principales opciones disponibles son:\n\nfm = “minres” para la solución residual mínima (el valor predeterminado).\nfm = “ml” para la solución de máxima verosimilitud.\nfm = “pa” para la solución del eje principal.\n\nHagamos el análisis factorial utilizando el método del eje principal y la rotación varimax. Los otros parámetros son la matriz de correlación y el número de factores nfactors = 3.\n\nLifEx_factor &lt;- fa(r = cor_mat, nfactors = 3, fm = \"ml\", rotate = \"varimax\")\n\nEl resultado del análisis factorial realizado anteriormente es:\n\nLifEx_factor\n\nFactor Analysis using method =  ml\nCall: fa(r = cor_mat, nfactors = 3, rotate = \"varimax\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n     ML1  ML2  ML3   h2     u2 com\nm0  0.96 0.12 0.23 1.00 0.0048 1.1\nm25 0.65 0.17 0.44 0.64 0.3617 1.9\nm50 0.43 0.35 0.79 0.93 0.0663 2.0\nm75 0.08 0.52 0.66 0.71 0.2877 1.9\nw0  0.97 0.22 0.08 1.00 0.0049 1.1\nw25 0.76 0.56 0.31 0.99 0.0111 2.2\nw50 0.54 0.73 0.40 0.98 0.0201 2.5\nw75 0.16 0.87 0.28 0.85 0.1460 1.3\n\n                       ML1  ML2  ML3\nSS loadings           3.37 2.08 1.64\nProportion Var        0.42 0.26 0.21\nCumulative Var        0.42 0.68 0.89\nProportion Explained  0.48 0.29 0.23\nCumulative Proportion 0.48 0.77 1.00\n\nMean item complexity =  1.8\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  28  with the objective function =  14.05\ndf of  the model are 7  and the objective function was  0.27 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.03 \n\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML3\nCorrelation of (regression) scores with factors   1.00 0.98 0.95\nMultiple R square of scores with factors          1.00 0.95 0.91\nMinimum correlation of possible factor scores     0.99 0.91 0.82\n\n\nHay mucha información aquí. Veamos algunas ideas relevantes:\n\nLa solución comienza con las cargas factoriales para los factores ML1, ML3 y ML2. Las cargas factoriales son los únicos valores afectados por la rotación. Para cada variable obtenemos las comunalidades h2 y unicidades u2.\nLas comunidades son bastante heterogéneas, oscilando entre 0,96 para m0 y 0,76 para w25.\nLa proporción de varianza explicada por cada factor se presenta en Proportion Var y la varianza acumulada en Cumulative Var. De la Var acumulativa aprendemos que el modelo explica el 54% de la varianza total. Este valor se considera bajo para un análisis factorial exploratorio.\n\nPodemos ver mejor el patrón de cargas factoriales imprimiéndolas con un valor de corte:\n\nprint(LifEx_factor$loadings, cutoff = 0.3)\n\n\nLoadings:\n    ML1   ML2   ML3  \nm0  0.964            \nm25 0.646       0.438\nm50 0.430 0.354 0.790\nm75       0.525 0.656\nw0  0.970            \nw25 0.764 0.556 0.310\nw50 0.536 0.729 0.401\nw75       0.867      \n\n                 ML1   ML2   ML3\nSS loadings    3.375 2.082 1.640\nProportion Var 0.422 0.260 0.205\nCumulative Var 0.422 0.682 0.887\n\n\nPodemos ver que podemos agrupar las variables m0 a m50 y w0 a w50 en un factor ML1, las variables m50, m75 y w25 a w75 en el factor ML2 y las variables m25 a m75 en el factor ML3. Para este conjunto de datos, este resultado está en correspondencia con el significado de las variables. ML1 estaría relacionado con la capacidad visual, ML3 con la capacidad verbal y ML2 con la capacidad matemática."
  },
  {
    "objectID": "Post/Multivariado/AnFact.html#puntuaciones-de-los-factores",
    "href": "Post/Multivariado/AnFact.html#puntuaciones-de-los-factores",
    "title": "Análisis Factorial",
    "section": "Puntuaciones de los factores",
    "text": "Puntuaciones de los factores\nMientras que las cargas factoriales ayudan a interpretar el significado de los factores, las puntuaciones factoriales permiten obtener valores de los factores para cada observación. Los argumentos de psych::factor.scores son las observaciones para las que queremos calcular las puntuaciones y el modelo de análisis factorial:\n\nLifEx_scores &lt;- factor.scores(LifEx |&gt; select(m0:w75), LifEx_factor)\n\nEl resultado de factor.scores es una lista con matrices:\n\npuntuaciones con los valores de las puntuaciones de los factores para cada observación.\nponderaciones con las ponderaciones utilizadas para obtener las puntuaciones.\nr.scores, la matriz de correlación de las puntuaciones.\n\nVeamos cómo se ven las puntuaciones de los factores:\n\nLifEx_scores$scores |&gt;\n  as_tibble() |&gt;\n  slice(1:5) |&gt;\n  kbl() |&gt;\n  kable_styling(full_width = FALSE)\n\n\n\n\nML1\nML2\nML3\n\n\n\n\n-0.2698331\n1.9021552\n1.9665810\n\n\n-2.7806603\n-0.6926360\n-1.9102875\n\n\n-2.8120358\n-0.8260575\n0.0178951\n\n\n0.1453850\n-0.2773256\n-0.8947034\n\n\n-0.1912897\n0.5225114\n-1.6371799\n\n\n\n\n\n\n\nHagamos un diagrama de dispersión de los factores ML1 y ML3, destacando el país de cada observación.\n\nscores &lt;- as_tibble(LifEx_scores$scores)\nLifEx$sexM &lt;-LifEx[2:5]\nLifEx$sexW &lt;-LifEx[6:9]\nLifEx$sex &lt;-LifEx[10:11]\nscores &lt;- bind_cols(LifEx |&gt; select(Country, sexM,sexW), scores) \n\nscores |&gt;\n  ggplot(aes(ML1, ML2, color = Country)) +\n  geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nEl gráfico muestra dos factores no correlacionados, distribuidos uniformemente entre los países."
  },
  {
    "objectID": "Post/Multivariado/PCA.html",
    "href": "Post/Multivariado/PCA.html",
    "title": "Análisis de Componentes Principales",
    "section": "",
    "text": "El análisis de componentes principales (PCA) en la programación R es un análisis de los componentes lineales de todos los atributos existentes. Los componentes principales son combinaciones lineales (transformación ortogonal) del predictor original en el conjunto de datos. Es una técnica útil para EDA (análisis exploratorio de datos) y permite visualizar mejor las variaciones presentes en un conjunto de datos con muchas variables.\nUna de las aplicaciones del PCA es la reducción de dimensionalidad (variables), perdiendo la menor cantidad de información (varianza) posible: cuando tenemos un gran número de variables cuantitativas posiblemente correlacionadas (indicativas de la existencia de información redundante), el PCA nos permite reducirlos a un número menor de variables transformadas (componentes principales) que explican gran parte de la variabilidad de los datos. Cada dimensión o componente principal generado por PCA será una combinación lineal de las variables originales, y además serán independientes o no correlacionadas entre sí. Los componentes principales generados pueden, a su vez, utilizarse en métodos de aprendizaje supervisado, como la regresión de componentes principales o mínimos cuadrados parciales.\nEl PCA también sirve como herramienta para la visualización de datos: supóngase que quisiéramos representar \\(n\\) observaciones con medidas sobre \\(p\\) variables \\((X=X1,X2,...,Xp)\\) como parte de un análisis exploratorio de los datos. Lo que podríamos hacer es examinar representaciones bidimensionales, sin embargo, existen un total de \\((p2)=p(p−1)/2\\) posibles representaciones entre pares de variables, y si el número de variables es muy alto, estas representaciones se harían inviables, además de que posiblemente la información contenida en cada una sería solo una pequeña fracción de la información total contenida en los datos.\nEl PCA puede considerarse como una rotación de los ejes del sistema de coordenadas de las variables originales a nuevos ejes ortogonales, de manera que estos ejes coincidan con la dirección de máxima varianza de los datos.\nNOTA: El PCA no requiere la suposición de normalidad multivariante de los datos."
  },
  {
    "objectID": "Post/Multivariado/PCA.html#descripción",
    "href": "Post/Multivariado/PCA.html#descripción",
    "title": "Análisis de Componentes Principales",
    "section": "Descripción",
    "text": "Descripción\nLos conjuntos de datos incluyen:\n\nClub: nombre del club\nCompetición: competición a la que pertenece el club\nTamaño del equipo\nValor de mercado\nValor de mercado de los jugadores\nMV mejores 18 jugadores\nProporción de VM"
  },
  {
    "objectID": "Post/Multivariado/PCA.html#conjunto-de-datos-de-los-100-mejores-clubes",
    "href": "Post/Multivariado/PCA.html#conjunto-de-datos-de-los-100-mejores-clubes",
    "title": "Análisis de Componentes Principales",
    "section": "Conjunto de datos de los 100 mejores clubes",
    "text": "Conjunto de datos de los 100 mejores clubes\nPara este análisis son 7 variables a estudiar, de las cuales 1 es cualitativa, mientras que las otras 6 son cuantitativas.\n\nlibrary(readr)\nvalue &lt;- read_csv(\"datos/value.csv\", \n     col_types = cols(Age = col_number(), \n         Squad_size = col_number(), Market_value = col_number(), \n         Market_value_of_players = col_number(), \n         MV_Top_18_players = col_number(), \n         Share_of_MV = col_number()))\nstr(value)\n\nspc_tbl_ [100 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Club                   : chr [1:100] \"Manchester City\" \"Paris Saint-Germain\" \"Manchester United\" \"Chelsea FC\" ...\n $ Competition            : chr [1:100] \"Premier League\" \"Ligue 1\" \"Premier League\" \"Premier League\" ...\n $ Age                    : num [1:100] 27.2 26.1 28 26.8 27 25.9 27.2 28.1 25.5 25.3 ...\n $ Squad_size             : num [1:100] 23 35 28 27 27 26 27 22 24 29 ...\n $ Market_value           : num [1:100] 1050 998 937 882 880 ...\n $ Market_value_of_players: num [1:100] 45.8 28.5 33.5 32.7 32.6 ...\n $ MV_Top_18_players      : num [1:100] 988 889 850 816 810 ...\n $ Share_of_MV            : num [1:100] 93.9 89.1 90.7 92.5 92.2 96.1 89.2 96.7 90.7 91.6 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Club = col_character(),\n  ..   Competition = col_character(),\n  ..   Age = col_number(),\n  ..   Squad_size = col_number(),\n  ..   Market_value = col_number(),\n  ..   Market_value_of_players = col_number(),\n  ..   MV_Top_18_players = col_number(),\n  ..   Share_of_MV = col_number()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "Post/Multivariado/PCA.html#diagrama-de-dispersión-y-correlaciones",
    "href": "Post/Multivariado/PCA.html#diagrama-de-dispersión-y-correlaciones",
    "title": "Análisis de Componentes Principales",
    "section": "Diagrama de dispersión y correlaciones",
    "text": "Diagrama de dispersión y correlaciones\n\nlibrary(psych)\npairs.panels(value[,-2],\n             gap = 0,\n             bg = c(\"red\",\n                    \"blue\")[value$Competition],\n             pch=21)"
  },
  {
    "objectID": "Post/Multivariado/PCA.html#paquete-r",
    "href": "Post/Multivariado/PCA.html#paquete-r",
    "title": "Análisis de Componentes Principales",
    "section": "Paquete R",
    "text": "Paquete R\n\nlibrary(readr)\nvalu &lt;- read_csv(\"datos/valu.csv\", col_types = cols(Age = col_number(), \n         Squad_size = col_number(), Market_value = col_number(), \n         Market_value_of_players = col_number(), \n         MV_Top_18_players = col_number(), \n         Share_of_MV = col_number()))\n\n\npc2 &lt;- prcomp(valu[,-1],\n             center = TRUE,\n            scale. = TRUE,na.rm = TRUE)\n\nWarning: In prcomp.default(valu[, -1], center = TRUE, scale. = TRUE, na.rm = TRUE) :\n extra argument 'na.rm' will be disregarded\n\nattributes(pc2)\n\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n\n\n\npc2$center\n\n                    Age              Squad_size            Market_value \n                26.3080                 27.5400                292.6927 \nMarket_value_of_players       MV_Top_18_players             Share_of_MV \n                10.8522                270.7508                 92.4180 \n\n\n\nprint(pc2)\n\nStandard deviations (1, .., p=6):\n[1] 1.75158822 1.26016158 0.99494242 0.58818062 0.08848976 0.01530459\n\nRotation (n x k) = (6 x 6):\n                               PC1         PC2         PC3         PC4\nAge                     -0.1125183  0.22705161  0.92882782  0.27029270\nSquad_size               0.1790218 -0.67843255 -0.01674992  0.70174818\nMarket_value            -0.5564697 -0.16914257 -0.04172734  0.04736752\nMarket_value_of_players -0.5659828 -0.07688986 -0.02898629 -0.07299739\nMV_Top_18_players       -0.5599041 -0.14437269 -0.05078645  0.07073222\nShare_of_MV             -0.1086425  0.65788457 -0.36309460  0.64955061\n                                  PC5          PC6\nAge                     -0.0001246218 -0.002813782\nSquad_size              -0.1210852523 -0.016962987\nMarket_value             0.4952310739 -0.642256604\nMarket_value_of_players -0.8087160455 -0.116451693\nMV_Top_18_players        0.2920320302  0.756834826\nShare_of_MV             -0.0279317132 -0.029169214"
  },
  {
    "objectID": "Post/Multivariado/PCA.html#resume",
    "href": "Post/Multivariado/PCA.html#resume",
    "title": "Análisis de Componentes Principales",
    "section": "Resume",
    "text": "Resume\n\nsummary(pc2)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6\nStandard deviation     1.7516 1.2602 0.9949 0.58818 0.08849 0.01530\nProportion of Variance 0.5113 0.2647 0.1650 0.05766 0.00131 0.00004\nCumulative Proportion  0.5113 0.7760 0.9410 0.99866 0.99996 1.00000"
  },
  {
    "objectID": "Post/Multivariado/PCA.html#ortogonalidad-de-las-pcs",
    "href": "Post/Multivariado/PCA.html#ortogonalidad-de-las-pcs",
    "title": "Análisis de Componentes Principales",
    "section": "Ortogonalidad de las PCs",
    "text": "Ortogonalidad de las PCs\n\npairs.panels(pc2$x,\n             gap=0,\n             bg = c(\"red\", \"blue\")[value$Competition],\n             pch=21)\n\n\n\n\nCálculo de la varianza total explicada por cada componente principal\n\n#calculate total variance explained by each principal component\nvar_explained = pc2$sdev^2 / sum(pc2$sdev^2)\n\n#create scree plot\nlibrary(ggplot2)\n\nqplot(c(1:6), var_explained) + \n  geom_line() + \n  xlab(\"Principal Component\") + \n  ylab(\"Variance Explained\") +\n  ggtitle(\"Scree Plot\") +\n  ylim(0, 1)\n\n\n\n\nSe observa que el PC1 explica alrededor del 51,13% d ela variabilidad y PC2 explica alrededor del 26,47% de la variabilidad.\nOtra forma de interpretar la trama es que PC1 está correlacionado positivamente con la variable Squad_size, y PC1 está correlacionado negativamente con Age, Market_value, Market_value_of_players, MV_Top_18_players, Share_of_MV.\nPC2 está correlacionado con Age y Share_of_MV, pero está correlacionado negativamente con Squad_size, Market_value, Market_value_of_players, MV_Top_18_players.\n\nlibrary(ggplot2)\nlibrary(usethis)\nlibrary(devtools)\ninstall_github(\"vqv/ggbiplot\")\nlibrary(ggbiplot)\ng &lt;- ggbiplot(pc2,\n              obs.scale = 1,\n              var.scale = 1,\n              groups = valu$Competition,\n              ellipse = TRUE,\n              circle = TRUE,\n              ellipse.prob = 0.68)\ng &lt;- g + scale_color_discrete(name = '')\ng &lt;- g + theme(legend.direction = 'horizontal',\n               legend.position = 'top')\nprint(g)\n\n\n\n\nBi plot es una herramienta importante en PCA para comprender lo que sucede en el conjunto de datos."
  },
  {
    "objectID": "Post/Multivariado/Python_Clustering.html",
    "href": "Post/Multivariado/Python_Clustering.html",
    "title": "Soul to Squeeze",
    "section": "",
    "text": "from scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist\nimport pandas as pd\nurl= \"diabetes.csv\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = pd.read_table(url, sep = \",\", names=names,header=1)\nprint(data.shape)\ndata.head()\n\n(767, 9)\n\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ntest\nmass\npedi\nage\nclass\n\n\n\n\n0\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n1\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n2\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n3\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n4\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster import hierarchy\nimport matplotlib.pyplot as plt\ny=data['class']\nX=data.iloc[:,0:8]\nscaler = StandardScaler()\nscaler.fit(X)\nStandardScaler(copy=True, with_mean=True, with_std=True)\nX= scaler.transform(X)\n#dist2=pairwise_distances(X)\nZ = hierarchy.linkage(X,'average')\nplt.figure(figsize=(20,15))\ndn = hierarchy.dendrogram(Z)\n\n\n\n\n\nimport seaborn as sns\nsns.set(color_codes=True)\ng = sns.clustermap(X, cmap=\"mako\",method='ward')\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import pairwise_distances\nimport numpy as np\nn_clusters = 2\nmodel = AgglomerativeClustering(n_clusters=n_clusters,linkage=\"average\")\nmodel.fit(X)\nclustlabels=model.fit_predict(X)\nunique_elements, counts_elements = np.unique(clustlabels, return_counts=True)\nprint(\"Size of the two clusters\")\nprint(np.asarray((unique_elements, counts_elements)))\n\nSize of the two clusters\n[[  0   1]\n [760   7]]\n\n\n\npd.crosstab(clustlabels,y)\n\n\n\n\n\n\n\nclass\n0\n1\n\n\nrow_0\n\n\n\n\n\n\n0\n498\n262\n\n\n1\n2\n5\n\n\n\n\n\n\n\n\nmodel = AgglomerativeClustering(n_clusters=n_clusters,linkage=\"ward\")\nmodel.fit(X)\nclustlabels=model.fit_predict(X)\nunique_elements, counts_elements = np.unique(clustlabels, return_counts=True)\nprint(\"Size of gthe two clusters\")\nprint(np.asarray((unique_elements, counts_elements)))\n\nSize of gthe two clusters\n[[  0   1]\n [354 413]]\n\n\n\npd.crosstab(clustlabels,y)\n\n\n\n\n\n\n\nclass\n0\n1\n\n\nrow_0\n\n\n\n\n\n\n0\n165\n189\n\n\n1\n335\n78\n\n\n\n\n\n\n\n\n# Plot clustering results\nfor index, metric in enumerate([\"cosine\", \"euclidean\", \"cityblock\"]):\n    model = AgglomerativeClustering(n_clusters=n_clusters,linkage=\"average\", affinity=metric)\n    model.fit(X)\n    plt.figure(figsize=(8,8))\n    plt.axes([0, 0, 1, 1])\n    for l, c in zip(np.arange(model.n_clusters), 'rgbk'):\n        plt.plot(X[model.labels_ == l].T, c=c, alpha=.5)\n    plt.axis('tight')\n    plt.axis('off')\n    plt.suptitle(\"AgglomerativeClustering(affinity=%s)\" % metric, size=20)\n\n\nplt.show()\n\nC:\\Users\\USUARIO\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\nC:\\Users\\USUARIO\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\nC:\\Users\\USUARIO\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn("
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Contenidos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nCategories\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nAnálisis Discriminante\n\n\nOscar Chullo Puclla\n\n\nR,Multivariado\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nAnálisis Factorial\n\n\nOscar Chullo Puclla\n\n\nR,Multivariado\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nAnálisis de Componentes Principales\n\n\nOscar Chullo Puclla\n\n\nR,PCA,Multivariado\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nAnálisis de Correspondencia Simple\n\n\nOscar Chullo Puclla\n\n\nR,Multivariado\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nClúster\n\n\nOscar Chullo Puclla\n\n\nR,Clúster,Multivariado\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#r",
    "href": "projects.html#r",
    "title": "Contenidos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nCategories\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nAnálisis Discriminante\n\n\nOscar Chullo Puclla\n\n\nR,Multivariado\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nAnálisis Factorial\n\n\nOscar Chullo Puclla\n\n\nR,Multivariado\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nAnálisis de Componentes Principales\n\n\nOscar Chullo Puclla\n\n\nR,PCA,Multivariado\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nAnálisis de Correspondencia Simple\n\n\nOscar Chullo Puclla\n\n\nR,Multivariado\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nClúster\n\n\nOscar Chullo Puclla\n\n\nR,Clúster,Multivariado\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#time-series",
    "href": "projects.html#time-series",
    "title": "Contenidos",
    "section": "Time Series",
    "text": "Time Series\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nhola mundo wey"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Contenidos",
    "section": "Project 2",
    "text": "Project 2\nprueba"
  },
  {
    "objectID": "projects.html#curriculum-vitae",
    "href": "projects.html#curriculum-vitae",
    "title": "Contenidos",
    "section": "Curriculum vitae",
    "text": "Curriculum vitae\nPuede visualizar mi curriculum vitae en:\nCV"
  },
  {
    "objectID": "Untitled/cv1.html",
    "href": "Untitled/cv1.html",
    "title": "Resumen",
    "section": "",
    "text": "Resumen\nSoy Estudiante de la Escuela Profesional de Matemáticas - Estadística, actualmente estoy en el noveno ciclo, aspiro a ser data science, tengo dominio de R, Python e inglés básico. Soy de naturaleza explorativa en aplicaciones de la estadistica.\n\n\nFormación Académica\n\n\n# A tibble: 1 × 5\n  what                     when                     with             where why  \n  &lt;chr&gt;                    &lt;chr&gt;                    &lt;chr&gt;            &lt;chr&gt; &lt;lis&gt;\n1 Matemática - Estadística Agosto 2019 - Actualidad Universidad Nac… Cusc… &lt;chr&gt;\n\n\n\n\nCursos y/o Estudios complementarios\n\n\n# A tibble: 4 × 5\n  what                         when                        with      where why  \n  &lt;chr&gt;                        &lt;chr&gt;                       &lt;chr&gt;     &lt;chr&gt; &lt;lis&gt;\n1 BootCamp en ciencia de datos Setiembre 2023 - Actualidad Data Gro… Actu… &lt;chr&gt;\n2 Inglés                       Setiembre 2020 - Julio 2022 Centro d… He a… &lt;chr&gt;\n3 Ofimática                    Marzo 2022 - Marzo 2022     Institut… He a… &lt;chr&gt;\n4 Habilidades Productivas      Actualidad                  Data Sci… Esto… &lt;chr&gt;\n\n\n\n\nLogros\n\n\n# A tibble: 4 × 5\n  what                        when           with                    where why  \n  &lt;chr&gt;                       &lt;chr&gt;          &lt;chr&gt;                   &lt;chr&gt; &lt;lis&gt;\n1 Aprender R                  \"Autodidacta\"  Aprendi R explorando a… &lt;NA&gt;  &lt;chr&gt;\n2 Aprender R markdown         \"Autodidacta \" Aprendi R markdown exp… &lt;NA&gt;  &lt;chr&gt;\n3 Creacion de blogs y website \"Autodidacta\"  Aprendi a crear y admi… &lt;NA&gt;  &lt;chr&gt;\n4 Programación en Python      \"Cursos\"       Estoy participando en … &lt;NA&gt;  &lt;chr&gt;\n\n\n\n\nPublicaciones\n\n\n\nC:/Users/USUARIO/AppData/Local/Temp/RtmpOEG2EI/file1b5c2f4f2c83.yaml"
  },
  {
    "objectID": "Post/Multivariado/Cluster.html",
    "href": "Post/Multivariado/Cluster.html",
    "title": "Clúster",
    "section": "",
    "text": "El análisis clúster es una técnica no supervisada, diseñada para clasificar las observaciones en grupos de tal forma que:\n\nCada grupo (conglomerado o clúster) sea homogéneo con respecto a las variables utilizadas para caracterizarlos; es decir, que cada observación contenida en él sea parecida a todas las que estén incluidas en dicho grupo. (principio de cohesión).\nQue los grupos sean lo más distintos posible unos de otros respecto a las variables consideradas. (principio de separación).\n\n\nHierarchical clustering\nHierarchical clustering es una alternativa a los métodos de partitioning clustering que no requiere que se pre-especifique el número de clusters.\nLos métodos que engloba el hierarchical clustering se subdividen en dos tipos con respecto a la estrategia que vaya a seleccionarse para crear los grupos:\n\nAgglomerative clustering (bottom-up): El agrupamiento inicia en la base del árbol, en el que cada observación forma un clúster individual. Los clusters se van combinado a medida que la estructura crece hasta converger en una única “rama” central.\nDivisive clustering (top-down): Es la estrategia opuesta al agglomerative clustering,puesto que en este caso se inicia con todas las observaciones contenidas en un mismo clúster y se suceden divisiones hasta que cada observación forma un cluster individual.\n\nEn ambos casos, los resultados se representan de forma muy intuitiva en una estructura de árbol llamada dendrograma.\n\n\nEjemplo Práctico\nLos siguientes datos simulados contienen observaciones que pertenecen a cuatro grupos distintos, en los que se pretende aplicar hierarchical clustering aglomerativo con el fin de identificarlos.\n\nlibrary(magrittr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nset.seed(123)\n# Se simulan datos aleatorios con dos dimensiones\ndatos &lt;- matrix(rnorm(n = 100*2), nrow = 100, ncol = 2,\n                dimnames = list(NULL,c(\"x\", \"y\")))\ndatos &lt;- as.data.frame(datos)\n\n# Se determina la media que va a tener cada grupo en cada una de las dos dimensiones. En total 2*4 medias. \n\n# Este valor se utiliza para separar cada grupo de los demás.\nmedia_grupos &lt;- matrix(rnorm(n = 8, mean = 0, sd = 4), nrow = 4, ncol = 2,\n                       dimnames = list(NULL, c(\"media_x\", \"media_y\")))\nmedia_grupos &lt;- as.data.frame(media_grupos)\nmedia_grupos &lt;- media_grupos %&gt;% mutate(grupo = c(\"a\",\"b\",\"c\",\"d\"))\n\n# Se genera un vector que asigne aleatoriamente cada observación a uno de\n# los 4 grupos\ndatos &lt;- datos %&gt;% mutate(grupo = sample(x = c(\"a\",\"b\",\"c\",\"d\"),\n                                         size = 100,\n                                         replace = TRUE))\n\n# Se incrementa el valor de cada observación con la media correspondiente al\n# grupo asignado.\ndatos &lt;- left_join(datos, media_grupos, by = \"grupo\")\ndatos &lt;- datos %&gt;% mutate(x = x + media_x,\n                          y = y + media_y)\ndatos &lt;- datos %&gt;% select(grupo, x, y)\nggplot(data = datos, aes(x = x, y = y, color = grupo)) +\n  geom_point(size = 2.5) +\n  theme_bw()\n\n\n\n\nAl aplicar un hierarchical clustering aglomerativo se tiene que escoger una medida de distancia (1-similitud) y un tipo de linkage.\nEn este caso, se emplea la función hclust(), a la que se pasa como argumento una matriz de distancia euclidea y el tipo de linkages. Se comparan los resultados con los linkages complete, single y average.\nDado que los datos fueron producto de una simulación, considerando que las dos dimensiones tienen aproximadamente la misma magnitud, no será necesario escalarlos ni centrarlos.\n\n# Se calculan las distancias\nmatriz_distancias &lt;- dist(x = datos[, c(\"x\", \"y\")], method = \"euclidean\")\nset.seed(567)\nhc_euclidea_completo &lt;- hclust(d = matriz_distancias, method = \"complete\")\nhc_euclidea_single   &lt;- hclust(d = matriz_distancias, method = \"single\")\nhc_euclidea_average  &lt;- hclust(d = matriz_distancias, method = \"average\")\n\nLos objetos devueltos por hclust() pueden representarse en forma de dendrograma con la función plot() o con la función fviz_dend() del paquete factoextra.\n\npar(mfrow = c(3,1))\nplot(x = hc_euclidea_completo, cex = 0.6, xlab = \"\", ylab = \"\", sub = \"\",\n     main = \"Distancia euclídea, Linkage complete\")\n\n\n\n\n\nplot(x = hc_euclidea_single, cex = 0.6, xlab = \"\", ylab = \"\", sub = \"\",\n     main = \"Distancia euclídea, Linkage single\")\n\n\n\n\n\nplot(x = hc_euclidea_average, cex = 0.6, xlab = \"\", ylab = \"\", sub = \"\",\n     main = \"Distancia euclídea, Linkage average\")\n\n\n\n\nEl conocer que existen 4 grupos, nos permite evaluar qué linkage consigue los mejores resultados. En este caso, los tres tipos identifican claramente 4 clusters, si bien esto no significa que en los 3 dendrogramas los clusters estén formados por exactamente las mismas observaciones.\nUna vez creado el dendrograma, se tiene que decidir a qué altura se corta para generar los clústers.\nLa función cutree() recibe como input un dendrograma y devuelve el cluster al que se ha asignado cada observación dependiendo del número de clusters especificado (argumento k) o la altura de corte indicada (argumento h).\n\ncutree(hc_euclidea_completo, k = 4)\n\n  [1] 1 2 2 3 3 2 2 3 2 3 1 4 2 2 3 4 4 3 4 2 4 3 4 3 3 2 4 1 2 4 4 1 4 1 4 2 1\n [38] 4 1 2 1 4 2 4 1 3 4 2 3 4 1 3 4 1 3 2 1 4 3 2 2 3 4 3 4 4 4 1 2 4 4 4 1 1\n [75] 2 1 2 1 4 4 1 3 3 2 4 1 1 4 3 1 1 1 4 2 1 2 4 1 1 1\n\n\n\ncutree(hc_euclidea_completo, h = 6)\n\n  [1] 1 2 2 3 3 2 2 3 2 3 1 4 2 2 3 4 4 3 4 2 4 3 4 3 3 2 4 1 2 4 4 1 4 1 4 2 1\n [38] 4 1 2 1 4 2 4 1 3 4 2 3 4 1 3 4 1 3 2 1 4 3 2 2 3 4 3 4 4 4 1 2 4 4 4 1 1\n [75] 2 1 2 1 4 4 1 3 3 2 4 1 1 4 3 1 1 1 4 2 1 2 4 1 1 1\n\n\nUna forma visual de comprobar los errores en las asignaciones es indicando en el argumento labels el grupo real al que pertenece cada observación. Si la agrupación resultante coincide con los grupos reales, entonces, dentro de cada clusters las labels serán las mismas.\n\nplot(x = hc_euclidea_completo, cex = 0.6, sub = \"\",\n     main = \"Distancia euclídea, Linkage complete, k=4\",\n     xlab = \"\", ylab = \"\", labels = datos[, \"grupo\"])\nabline(h = 6, lty = 2)\n\n\n\n\n\ntable(cutree(hc_euclidea_completo, h = 6), datos[, \"grupo\"])\n\n   \n     a  b  c  d\n  1  0  0  5 22\n  2 23  0  0  0\n  3  0  0 19  0\n  4  1 26  0  4\n\n\nEl método de hierarchical clustering aglomerativo con linkage completo y k=4 ha sido capaz de agrupar correctamente todas las observaciones, pudiendo identificarse los 4 grupos."
  }
]